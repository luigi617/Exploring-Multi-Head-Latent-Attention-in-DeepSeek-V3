# ExploringÂ Multiâ€‘HeadÂ LatentÂ Attention inÂ DeepSeekâ€‘Coder
*A systematic benchmark of LoRA,Â QLoRA & baseline fineâ€‘tuning across multiple attention kernels*

---

## âœ¨Â Project Motivation
Largeâ€‘languageâ€‘model fineâ€‘tuning is often constrained by **GPU memory**, **training speed**, and **engineering complexity**.â€¯This project delivers a *dropâ€‘in* framework to **measure the realâ€‘world tradeâ€‘offs** between adapter techniques (Baseline,â€¯LoRA,â€¯QLoRA) under a family of attention implementations:

* **Eager / SDPA** (PyTorch standard)
* **FlashAttentionâ€‘2**
* **PagedÂ Attention**
* **Multiâ€‘HeadÂ LatentÂ AttentionÂ (MLA)**

All experiments use **DeepSeekâ€‘Coderâ€‘1.3B** and a 120â€‘sample slice of **OpenOrca** for fast iteration during the midâ€‘point review.Â We are *not* reâ€‘implementing kernelsâ€”just benchmarking.

---

## ğŸ—‚Â Repository Outline
```text
.
â”œâ”€â”€ attentions/                 # Custom kernels (mla.py)
â”œâ”€â”€ configs/                    # JSON experiment configs (batchâ€‘size, kernels, â€¦)
â”œâ”€â”€ deepseekâ€‘models/            # Git subâ€‘module with patched DeepSeekâ€‘Coderâ€‘1.3B
â”œâ”€â”€ benchmark_attn.py           # Baseline    (fp16) benchmark script
â”œâ”€â”€ benchmark_attn_lora.py      # LoRA        benchmark script
â”œâ”€â”€ benchmark_attn_qlora.py     # QLoRA       benchmark script (v1)
â”œâ”€â”€ benchmark_attn_qlora_v2.py  # QLoRA       benchmark w/ profiling + ROUGE (v2)
â”œâ”€â”€ benchmark_wandb.py          # Thin wrapper that streams metrics to WeightsÂ &Â Biases
â”œâ”€â”€ finetune_deepseek_attn.py   # Oneâ€‘off fineâ€‘tune helper (no benchmarking)
â”œâ”€â”€ finetune_wandb.py           # Same as above but logs toÂ wandb
â”œâ”€â”€ model_FA.py                 # FlashAttentionâ€‘2 convenience loader
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # â† this file
```

---

## âš™ï¸Â Setup & Requirements
```bash
# 1â€†Â·Â Create environment
conda create -n deepseek-bench python=3.10
conda activate deepseek-bench

# 2â€†Â·Â Install deps (GPU, CUDAÂ 11.8+)
pip install -r requirements.txt

# 3â€†Â·Â (Optional) install Flashâ€‘Attentionâ€‘2 wheels if missing
#     see: https://github.com/Dao-AILab/flash-attention
```

---

## ğŸš€Â How to RunÂ â€“ Quick Examples
### 1.Â Baseline (fullâ€‘precision)
```bash
python benchmark_attn.py deepseek-ai/deepseek-coder-1.3b-base configs/baseline.json runs/baseline
```

### 2.Â LoRA
```bash
python benchmark_attn_lora.py deepseek-ai/deepseek-coder-1.3b-base configs/comparison.json runs/lora
```

### 3.Â QLoRAÂ (v2, profiling enabled)
```bash
python benchmark_attn_qlora_v2.py deepseek-ai/deepseek-coder-1.3b-base configs/comparison.json runs/qlora_v2
```

### 4.Â Stream metrics to WeightsÂ &Â Biases
```bash
export WANDB_ENTITY=<yourâ€‘entity>
export WANDB_PROJECT=deepseek-attn-bench
export WANDB_API_KEY=<yourâ€‘key>

python benchmark_attn_qlora_v2.py deepseek-ai/deepseek-coder-1.3b-base configs/comparison.json runs/bench

```

Our results can be found in the [public report](https://wandb.ai/louiszh-columbia-university/finetune_with_tables/reports/HPML-Adapter-Benchmarking-Results--VmlldzoxMjY4MTQzMA).

---

## ğŸ“ŠÂ Sample Results
| Mode      | Kernel            | TrainÂ tâ€¯â†“ | Tok/sâ€¯â†‘ | Peakâ€¯MiBâ€¯â†“ | ValÂ PPLâ€¯â†“ |
|-----------|-------------------|-----------|---------|------------|-----------|
| baseline  | eager             | 26.0Â s    | 3.87    | 13â€¯114     | 17.9      |
| baseline  | flashâ€‘attnâ€‘2      | 25.8Â s    | 3.90    | 13â€¯114     | 17.8      |
| baseline  | MLA               | 21.9Â s    | 4.60    | 16â€¯863     | 18.4      |
| **LoRA**  | eager             | 23.9Â s    | 4.22    | 16â€¯863     | 18.0      |
| **QLoRA** | flashâ€‘attnâ€‘2      | 18.7Â s    | 4.95    | **8â€¯732**  | 18.3      |

Detailed perâ€‘step profiling (CPU+CUDA) is saved in `runs/**/profiling_trace.json`; open in Chrome DevTools âœ *Performance* for flameâ€‘charts.

---

## ğŸ”Â Observations (Midâ€‘point)
* **FlashAttentionâ€‘2** gives a consistent ~4â€¯% speedâ€‘up over SDPA with no memory penalty.
* **MLA** delivers the highest throughput but increases GPU memory via latent projections.
* **QLoRAÂ +Â FAâ€‘2** *halves* peak memory vs. fp16 while also being the fastest overall.
* Quality differences (Perplexity, ROUGEâ€‘L) remain within Â±3â€¯% on the small OpenOrca slice.

> *Takeâ€‘away:* For smallâ€‘/midâ€‘scale fineâ€‘tuning, **QLoRA combined with FlashAttentionâ€‘2** provides the best computeâ€‘/memoryâ€‘efficiency sweet spot.

---

## ğŸ”„Â Reproduce & Extend
1. **Change kernels** â€“ edit `configs/*.json` â†’ "attn_impls" : ["eager", "flash_attention_2", â€¦].
2. **Bigger data**    â€“ bump `TRAIN_SIZE`, `VAL_SIZE`, or point to a HF dataset split.
3. **More epochs**    â€“ adjust `num_train_epochs` in the config or script.
4. **Distributed**    â€“ pass `--deepspeed ds_config.json` or enable FSDP in `TrainingArguments`.

---

## ğŸ“„Â License & Citation
Released under **ApacheÂ 2.0**.Â If you build on this work, please cite the original LoRA,Â QLoRA, FlashAttentionâ€‘2, and DeepSeekâ€‘Coder papers.

---
